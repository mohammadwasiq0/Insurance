{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Science Pipelne**\n",
    "1. **Data Ingestion**\n",
    "2. **Data Validation**\n",
    "3. **Model Training**\n",
    "4. **Model Analysis Validation**\n",
    "5. **Model Deployment**\n",
    "\n",
    "\n",
    "# **ML Project Insurance**\n",
    "\n",
    "## **Problem Statement**\n",
    "**The purpose of this data is to look into the different features to observe their relationship, ML model is based on several features of individuals such as age, physical/family condition, and location against their existing medical expense to be used for predicting future medical expenses of individuals that help medical insurance to make a decision on charging the premium.**\n",
    "\n",
    "## **Tech Stack Used**\n",
    "* **Python Modular Coding**\n",
    "* **Machine Learning**\n",
    "* **MongoDB Database**\n",
    "* **AWS Cloud**\n",
    "* **Apache**\n",
    "* **Docker**\n",
    "* **Grafana**\n",
    "* **DVC**\n",
    "* **MLFLOW**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part- 1**\n",
    "\n",
    "## **GitHub**\n",
    "1. Create a new GitHub Repository\n",
    "2. Choose the repository **`Public`** or **`Private`** \n",
    "3. Add a **`README.md`** file\n",
    "4. Add a **`.gitignore`** file and Choose **`Python`**\n",
    "5. Chosse a license, generally we choose the **`Apache license 2.0`**\n",
    "6. Now our repository is created.\n",
    "7. Click on **`<> Code`** and copy the `repository link`\n",
    "8. Open the **`Command Prompt`**\n",
    "9. Copy the **`Project Folder`** and paste that in Command Prompt,\n",
    "10. Run the command in command prompt\n",
    "```bash\n",
    "git clone https://github.com/\n",
    "```\n",
    "11. To Open **`VS Code`**, run the following command\n",
    "```bash\n",
    "code .\n",
    "```\n",
    "\n",
    "12. Get the data into the same folder.\n",
    "\n",
    "### **Now start to write the code**\n",
    "1. Create a new file and name it **`data_dump.py`** and write the folllowing code\n",
    "\n",
    "```python\n",
    "import pymongo # pip install pymongo\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://gl0427:wasiq123@cluster0.afuprqr.mongodb.net/?retryWrites=true&w=majority\")\n",
    "\n",
    "DATA_FILE_PATH= (r\"E:/wasiq/Insurance_Project/insurance.csv\")\n",
    "DATABASE_NAME= \"INSURANCE\"\n",
    "COLLECTION_NAME= \"INSURANCE_PROJECT\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df= pd.read_csv(DATA_FILE_PATH)\n",
    "    print(f\"Rows and Columns: {df.shape}\")\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    json_record = list(json.loads(df.T.to_json()).values())\n",
    "    print(json_record[0])\n",
    "\n",
    "    client[DATABASE_NAME][COLLECTION_NAME].insert_many(json_record)\n",
    "```    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pymongo # pip install pymongo\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "client = pymongo.MongoClient(\"<mongodb_url>\")\n",
    "\n",
    "DATA_FILE_PATH= (r\"DataFilePath\")\n",
    "DATABASE_NAME= \"INSURANCE\"\n",
    "COLLECTION_NAME= \"INSURANCE_PROJECT\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df= pd.read_csv(DATA_FILE_PATH)\n",
    "    print(f\"Rows and Columns: {df.shape}\")\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    json_record = list(json.loads(df.T.to_json()).values())\n",
    "    print(json_record[0])\n",
    "\n",
    "    client[DATABASE_NAME][COLLECTION_NAME].insert_many(json_record)\n",
    "    \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new file and name it **`template.py`** and write the folllowing code\n",
    "```python\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level = logging.INFO,\n",
    "    format = \"[%(asctime)s: %(levelname)s]: %(message)s\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    project_name = input(\"Enter your project name: \")\n",
    "    if project_name !='':\n",
    "        break\n",
    "\n",
    "logging.info(f\"Creating project by name: {project_name}\")\n",
    "\n",
    "list_of_files = [\n",
    "    \".github/workflows/.gitkeep\",\n",
    "    \".github/workflows/main.yaml\",\n",
    "    # f\"src/{project_name}/__init__.py\",\n",
    "    f\"{project_name}/__init__.py\",\n",
    "    f\"{project_name}/components/__init__.py\",\n",
    "    f\"{project_name}/components/data_ingestion.py\", \n",
    "    f\"{project_name}/components/data_transformation.py\",\n",
    "    f\"{project_name}/components/data_validation.py\",\n",
    "    f\"{project_name}/components/data_evaluation.py\",\n",
    "    f\"{project_name}/components/data_pusher.py\",\n",
    "    f\"{project_name}/components/data_trainer.py\",\n",
    "    f\"{project_name}/entity/__init__.py\",\n",
    "    f\"{project_name}/exception/__init__.py\",\n",
    "    f\"{project_name}/pipeline/__init__.py\",\n",
    "    f\"{project_name}/logger/__init__.py\",\n",
    "    f\"{project_name}/config.py\",\n",
    "    f\"{project_name}/exception.py\",\n",
    "    f\"{project_name}/predictor.py\",\n",
    "    f\"{project_name}/utils.py\",\n",
    "    f\"{project_name}/entity/__init__.py\",\n",
    "    f\"{project_name}/entity/artifact_entity.py\",\n",
    "    f\"{project_name}/entity/config_entity.py\",\n",
    "    f\"configs/config.yaml\",\n",
    "    \"requirements.txt\",\n",
    "    \"setup.py\",\n",
    "    \"main.py\"\n",
    "    \"data_dump.py\",\n",
    "    \"README.md\"\n",
    "]\n",
    "\n",
    "\n",
    "for filepath in list_of_files:\n",
    "    filepath = Path(filepath)\n",
    "    filedir, filename = os.path.split(filepath)\n",
    "    if filedir !=\"\":\n",
    "        os.makedirs(filedir, exist_ok=True)\n",
    "        logging.info(f\"Creating a new directory at : {filedir} for file: {filename}\")\n",
    "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
    "        with open(filepath, \"w\") as f:\n",
    "            pass\n",
    "            logging.info(f\"Creating a new file: {filename} for path: {filepath}\")\n",
    "    else:\n",
    "        logging.info(f\"file is already present at: {filepath}\")\n",
    "```\n",
    "\n",
    "It ask the name of the project\n",
    "```bash\n",
    "Enter project name: project_name\n",
    "```\n",
    "\n",
    "It Create all required files and folders for the project.\n",
    "\n",
    "Run the following command:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"folder structure and dump in database completed\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part - 2**\n",
    "### **Project Building**\n",
    "\n",
    "* **Setup file understanding & Implementation**\n",
    "* **Logger understanding & Implementation**\n",
    "* **Exception understanding & Implementation**\n",
    "\n",
    "## Start the coding in **`setup.py`**\n",
    "```python\n",
    "from setuptools import find_packages, setup\n",
    "from typing import List\n",
    "\n",
    "requriment_file_name = \"requirements.txt\"\n",
    "REMOVE_PACKAGE = \"-e .\"\n",
    "\n",
    "def get_requirements()->List[str]:\n",
    "    with open(requriment_file_name) as requirement_file:\n",
    "        requriment_list = requirement_file.readline()\n",
    "    requriment_list = [requriment_name.replace(\"\\n\", \"\") for requriment_name in requriment_list]\n",
    "\n",
    "    if REMOVE_PACKAGE in requriment_list:\n",
    "        requriment_list.remove(REMOVE_PACKAGE)\n",
    "    return requriment_list\n",
    "\n",
    "\n",
    "\n",
    "setup(name='Insurance',\n",
    "      version='0.0.1',\n",
    "      description='Insurance Industry lavel project',\n",
    "      author='Mohammad Wasiq',\n",
    "      author_email='mohammadwasiq0786@gmail.com',\n",
    "      packages=find_packages(),\n",
    "      install_reqires = get_requirements()\n",
    "     )\n",
    "```\n",
    "\n",
    "## Write the **`requirements.txt`** file\n",
    "```python\n",
    "dnspython==2.2.1\n",
    "fastapi==0.78.0\n",
    "httptools==0.5.0\n",
    "imblearn==0.0\n",
    "pip-chill==1.0.1\n",
    "#python-dotenv==0.21.0\n",
    "uvicorn==0.18.3\n",
    "watchfiles==0.17.0\n",
    "websockets==10.3\n",
    "wincertstore==0.2\n",
    "xgboost==1.6.2\n",
    "pandas\n",
    "PyYAML\n",
    "numpy\n",
    "scikit-learn\n",
    "apache-airflow\n",
    "-e .\n",
    "```\n",
    "\n",
    "Install the requirements.txt\n",
    "```python\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### **Update the GitHub repository**\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"setup file completed\"\n",
    "git push origin main\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the following code into **`logger`** folder's **`___init__.py`** file\n",
    "```python\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Creating logs directory to store log in files\n",
    "LOG_DIR = \"Insurance_log\"\n",
    "\n",
    "# Creating file name for log file based on current timestamp\n",
    "CURRENT_TIME_STAMP = f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Here, We are going to define the path to store log with folder_name\n",
    "LOG_FIlE_NAME = f\"log_{CURRENT_TIME_STAMP}.log\"\n",
    "\n",
    "\n",
    "#Creating LOG_DIR if it does not exists.\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "#Creating file path for projects.\n",
    "LOG_FIlE_PATH = os.path.join(LOG_DIR, LOG_FIlE_NAME)\n",
    "\n",
    "# If you want to read log select baseconfig and press f12 from your system.\n",
    "logging.basicConfig(filename=LOG_FIlE_PATH,\n",
    "filemode = \"w\",\n",
    "format = '[%(asctime)s] %(name)s - %(levelname)s - %(message)s',\n",
    "#level=logging.INFO,\n",
    "level=logging.DEBUG,\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the following code into **`exception`** folder's **`___init__.py`** file\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "class InsuranceException(Exception):\n",
    "    \n",
    "\n",
    "    def __init__(self, error_message:Exception, error_detail:sys):\n",
    "        super().__init__(error_message)\n",
    "        self.error_message = InsuranceException.error_message_detail(error_message, error_detail=error_detail)\n",
    "\n",
    "    @staticmethod\n",
    "    def error_message_detail(error:Exception, error_detail:sys)->str:\n",
    "        \"\"\"\n",
    "        error: Exception object raise from module\n",
    "        error_detail: is sys module contains detail information about system execution information.\n",
    "        \"\"\"\n",
    "        _, _, exc_tb = error_detail.exc_info()\n",
    "        line_number = exc_tb.tb_frame.f_lineno\n",
    "    \n",
    "        #extracting file name from exception traceback\n",
    "        file_name = exc_tb.tb_frame.f_code.co_filename \n",
    "\n",
    "        #preparing error message\n",
    "        error_message = f\"Error occurred python script name [{file_name}]\" \\\n",
    "                        f\" line number [{exc_tb.tb_lineno}] error message [{error}].\"\n",
    "\n",
    "        return error_message\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Formating how a object should be visible if used in print statement.\n",
    "        \"\"\"\n",
    "        return self.error_message\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Formating object of AppException\n",
    "        \"\"\"\n",
    "        return InsuranceException.__name__.__str__()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the following code into **`main.py`** file\n",
    "```python\n",
    "from Insurance.logger import logging\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.utils import get_collection_as_dataframe\n",
    "import sys, os\n",
    "from Insurance.entity.config_entity import DataIngestionConfig\n",
    "from Insurance.entity import config_entity\n",
    "from Insurance.components.data_ingestion import DataIngestion\n",
    "from Insurance.components.data_validation import DataValidation\n",
    "from Insurance.components.model_trainer import ModelTrainer\n",
    "from Insurance.components.data_transformation import DataTransformation\n",
    "from Insurance.components.model_evaluation import ModelEvaluation\n",
    "from Insurance.components.model_pusher import ModelPusher\n",
    "\n",
    "#def test_logger_and_expection():\n",
    "   # try:\n",
    "       # logging.info(\"Starting the test_logger_and_exception\")\n",
    "        #result = 3/0\n",
    "       # print(result)\n",
    "       # logging.info(\"Stoping the test_logger_and_exception\")\n",
    "    #except Exception as e:\n",
    "      #  logging.debug(str(e))\n",
    "       # raise InsuranceException(e, sys)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "     try:\n",
    "          #start_training_pipeline()\n",
    "          #test_logger_and_expection()\n",
    "       # get_collection_as_dataframe(database_name =\"INSURANCE\", collection_name = 'INSURANCE_PROJECT')\n",
    "       training_pipeline_config = config_entity.TrainingPipelineConfig()\n",
    "       \n",
    "      #data ingestion\n",
    "       data_ingestion_config  = config_entity.DataIngestionConfig(training_pipeline_config=training_pipeline_config)\n",
    "       print(data_ingestion_config.to_dict())\n",
    "       data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "       data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\n",
    "       \n",
    "       # Data Validation\n",
    "       data_validation_config = config_entity.DataValidationConfig(training_pipeline_config=training_pipeline_config)\n",
    "       data_validation = DataValidation(data_validation_config=data_validation_config,\n",
    "                         data_ingestion_artifact=data_ingestion_artifact)\n",
    "        \n",
    "       data_validation_artifact = data_validation.initiate_data_validation()\n",
    "\n",
    "      #Data Transformation\n",
    "\n",
    "       data_transformation_config = config_entity.DataTransformationConfig(training_pipeline_config=training_pipeline_config)\n",
    "       data_transformation = DataTransformation(data_transformation_config=data_transformation_config, \n",
    "       data_ingestion_artifact=data_ingestion_artifact)\n",
    "       data_transformation_artifact = data_transformation.initiate_data_transformation()\n",
    "\n",
    "\n",
    "      # Model Trainer \n",
    "       model_trainer_config = config_entity.ModelTrainingConfig(training_pipeline_config = training_pipeline_config)\n",
    "       model_trainer = ModelTrainer(model_trainer_config=model_trainer_config, data_transformation_artifact=data_transformation_artifact)\n",
    "       model_trainer_artifact = model_trainer.initiate_model_trainer()\n",
    "\n",
    "\n",
    "      # Model Evaluation\n",
    "       model_eval_config = config_entity.ModelEvaluationConfig(training_pipeline_config = training_pipeline_config)\n",
    "       model_eval = ModelEvaluation(model_eval_config = model_eval_config,\n",
    "       data_ingestion_artifact = data_ingestion_artifact,\n",
    "       data_transformation_artifact = data_transformation_artifact,\n",
    "       model_trainer_artifact = model_trainer_artifact)\n",
    "       model_evl_artifact = model_eval.intitate_model_evaluation()\n",
    "\n",
    "\n",
    "      # Model Pusher\n",
    "       model_pusher_config = config_entity.ModelPusherConfig(training_pipeline_config=training_pipeline_config)\n",
    "       model_pusher = ModelPusher(model_pusher_config=model_pusher_config,\n",
    "                     data_transformation_artifact= data_transformation_artifact,\n",
    "                     model_trainer_artifact= model_trainer_artifact)\n",
    "       Model_pusher_artifact = model_pusher.initiate_model_pusher()\n",
    "\n",
    "\n",
    "     except Exception as e:\n",
    "          print(e)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 3\n",
    "## Write the following code in **`utils.py`** file of Insurance Folder\n",
    "```py\n",
    "import pandas as pd\n",
    "from Insurance.logger import logging\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.config import mongo_client\n",
    "import os,sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import dill\n",
    "\n",
    "def get_collection_as_dataframe(database_name:str,collection_name:str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description: This function return collection as dataframe\n",
    "    =========================================================\n",
    "    Params:\n",
    "    database_name: database name\n",
    "    collection_name: collection name\n",
    "    =========================================================\n",
    "    return Pandas dataframe of a collection\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Reading data from database: {database_name} and collection: {collection_name}\")\n",
    "        df = pd.DataFrame(list(mongo_client[database_name][collection_name].find()))\n",
    "        logging.info(f\"Found columns: {df.columns}\")\n",
    "        if \"_id\" in df.columns:\n",
    "            logging.info(f\"Dropping column: _id \")\n",
    "            df = df.drop(\"_id\",axis=1)\n",
    "        logging.info(f\"Row and columns in df: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise InsuranceException(e, sys)\n",
    "\n",
    "#***********************************## Data_Validation*******************************************\n",
    "def write_yaml_file(file_path,data:dict):\n",
    "    try:\n",
    "        file_dir = os.path.dirname(file_path)\n",
    "        os.makedirs(file_dir,exist_ok=True)\n",
    "        with open(file_path,\"w\") as file_writer:\n",
    "            yaml.dump(data,file_writer)\n",
    "    except Exception as e:\n",
    "        raise InsuranceException(e, sys)\n",
    "\n",
    "def convert_columns_float(df:pd.DataFrame,exclude_columns:list)->pd.DataFrame:\n",
    "    try:\n",
    "        for column in df.columns:\n",
    "            if column not in exclude_columns:\n",
    "                if df[column].dtypes != 'O':\n",
    "                    df[column]=df[column].astype('float')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "#*********************************** Data_Transformation*******************************************\n",
    "\n",
    "def save_object(file_path: str, obj: object) -> None:\n",
    "    try:\n",
    "        logging.info(\"Entered the save_object method of utils\")\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            dill.dump(obj, file_obj)\n",
    "        logging.info(\"Exited the save_object method of utils\")\n",
    "    except Exception as e:\n",
    "        raise InsuranceException(e, sys) from e\n",
    "\n",
    "    \n",
    "def load_object(file_path: str, ) -> object:\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise Exception(f\"The file: {file_path} is not exists\")\n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            return dill.load(file_obj)\n",
    "    except Exception as e:\n",
    "        raise InsuranceException(e, sys) from e\n",
    "\n",
    "def save_numpy_array_data(file_path: str, array: np.array):\n",
    "    \"\"\"\n",
    "    Save numpy array data to file\n",
    "    file_path: str location of file to save\n",
    "    array: np.array data to save\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            np.save(file_obj, array)\n",
    "    except Exception as e:\n",
    "        raise InsuranceException(e, sys) from e\n",
    "\n",
    "#***********************************## Model Training*******************************************\n",
    "\n",
    "def load_numpy_array_data(file_path: str) -> np.array:\n",
    "    \"\"\"\n",
    "    load numpy array data from file\n",
    "    file_path: str location of file to load\n",
    "    return: np.array data loaded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            return np.load(file_obj)\n",
    "    except Exception as e:\n",
    "        raise InsuranceException(e, sys) from e\n",
    "```        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a file **`.env`** and write the following code\n",
    "```python\n",
    "MONGO_DB_URL= \"mongodb+srv://gl0427:wasiq123@cluster0.afuprqr.mongodb.net/?retryWrites=true&w=majority\"\n",
    "```\n",
    "\n",
    "## Write the following code in **`__init__.py`** file of Insurance Folder\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "print(f\"loading env variable from .env\")\n",
    "load_dotenv()\n",
    "```\n",
    "## Write the following code in **`config.py`** file of Insurance Folder\n",
    "```python\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "# Provide the mongodb localhost url to connect python to mongodb.\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class EnvironmentVariable:\n",
    "    mongo_db_url:str = os.getenv(\"MONGO_DB_URL\")\n",
    "    #aws_access_key_id:str = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    #aws_access_secret_key:str = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "\n",
    "env_var = EnvironmentVariable()\n",
    "mongo_client = pymongo.MongoClient(env_var.mongo_db_url)\n",
    "TARGET_COLUMN = \"expenses\"\n",
    "print(env_var.mongo_db_url)\n",
    "```\n",
    "### Run **`main.py`** file\n",
    "\n",
    "### GitHub Commands are as follows:\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m \"Utils file config file updated\"\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "## Creaate a Folder and named **`entity`** in Insurance Folder\n",
    "\n",
    "### Creatre a file **`__init__.py`** in entity folder\n",
    "### Creatre a file **`artifact_entity.py`** in entity folder and write the following code\n",
    "``` python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionArtifact:\n",
    "    feature_store_file_path:str\n",
    "    train_file_path:str \n",
    "    test_file_path:str\n",
    "\n",
    "@dataclass\n",
    "class DataValidationArtifact:\n",
    "    report_file_path:str\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationArtifact:\n",
    "    transform_object_path:str\n",
    "    transformed_train_path:str\n",
    "    transformed_test_path:str\n",
    "    target_encoder_path:str\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerArtifact:\n",
    "    model_path:str \n",
    "    r2_train_score:float \n",
    "    r2_test_score:float\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationArtifact:\n",
    "    is_model_accepted:bool\n",
    "    improved_accuracy:float\n",
    "\n",
    "@dataclass\n",
    "class ModelPusherArtifact:\n",
    "    pusher_model_dir:str \n",
    "    saved_model_dir:str\n",
    "```\n",
    "### Creatre a file **`config_entity.py`** in entity folder and write the following code\n",
    "``` python\n",
    "import os,sys\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.logger import logging\n",
    "from datetime import datetime\n",
    "\n",
    "FILE_NAME = \"insurance.csv\"\n",
    "TRAIN_FILE_NAME = \"train.csv\"\n",
    "TEST_FILE_NAME = \"test.csv\"\n",
    "TRANSFORMER_OBJECT_FILE_NAME = \"transformer.pkl\"\n",
    "MODEL_FILE_NAME = \"model.pkl\"\n",
    "\n",
    "TARGET_ENCODER_OBJECT_FILE_NAME = \"target_encoder.pkl\"\n",
    "\n",
    "class TrainingPipelineConfig:\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.artifact_dir = os.path.join(os.getcwd(),\"artifact\",f\"{datetime.now().strftime('%m%d%Y__%H%M%S')}\")\n",
    "        except Exception  as e:\n",
    "            raise InsuranceException(e,sys)    \n",
    "\n",
    "\n",
    "class DataIngestionConfig:\n",
    "    \n",
    "    def __init__(self,training_pipeline_config:TrainingPipelineConfig):\n",
    "        try:\n",
    "            self.database_name=\"INSURANCE\"\n",
    "            self.collection_name=\"INSURANCE_PROJECT\"\n",
    "            self.data_ingestion_dir = os.path.join(training_pipeline_config.artifact_dir , \"data_ingestion\")\n",
    "            self.feature_store_file_path = os.path.join(self.data_ingestion_dir,\"feature_store\",FILE_NAME)\n",
    "            self.train_file_path = os.path.join(self.data_ingestion_dir,\"dataset\",TRAIN_FILE_NAME)\n",
    "            self.test_file_path = os.path.join(self.data_ingestion_dir,\"dataset\",TEST_FILE_NAME)\n",
    "            self.test_size = 0.2\n",
    "        except Exception  as e:\n",
    "            raise InsuranceException(e,sys)      \n",
    "\n",
    "            \n",
    "# Convert data into dict\n",
    "    def to_dict(self,)->dict:\n",
    "        try:\n",
    "            return self.__dict__\n",
    "        except Exception  as e:\n",
    "            raise InsuranceException(e,sys)          \n",
    "\n",
    "class DataValidationConfig:\n",
    "    \n",
    "    def __init__(self,training_pipeline_config:TrainingPipelineConfig):\n",
    "        self.data_validation_dir = os.path.join(training_pipeline_config.artifact_dir , \"data_validation\")\n",
    "        self.report_file_path=os.path.join(self.data_validation_dir, \"report.yaml\")\n",
    "        self.missing_threshold:float = 0.2\n",
    "        self.base_file_path = os.path.join(\"insurance.csv\")\n",
    "\n",
    "\n",
    "class DataTransformationConfig:\n",
    "    \n",
    "    def __init__(self,training_pipeline_config:TrainingPipelineConfig):\n",
    "        self.data_transformation_dir = os.path.join(training_pipeline_config.artifact_dir , \"data_transformation\")\n",
    "        self.transform_object_path = os.path.join(self.data_transformation_dir,\"transformer\",TRANSFORMER_OBJECT_FILE_NAME)\n",
    "        self.transformed_train_path =  os.path.join(self.data_transformation_dir,\"transformed\",TRAIN_FILE_NAME.replace(\"csv\",\"npz\"))\n",
    "        self.transformed_test_path =os.path.join(self.data_transformation_dir,\"transformed\",TEST_FILE_NAME.replace(\"csv\",\"npz\"))\n",
    "        self.target_encoder_path = os.path.join(self.data_transformation_dir,\"target_encoder\",TARGET_ENCODER_OBJECT_FILE_NAME)\n",
    "\n",
    "\n",
    "class ModelTrainerConfig:\n",
    "    \n",
    "    def __init__(self,training_pipeline_config:TrainingPipelineConfig):\n",
    "        self.model_trainer_dir = os.path.join(training_pipeline_config.artifact_dir , \"model_trainer\")\n",
    "        self.model_path = os.path.join(self.model_trainer_dir,\"model\",MODEL_FILE_NAME)\n",
    "        self.expected_score = 0.7\n",
    "        self.overfitting_threshold = 0.3 # overfiting score\n",
    "\n",
    "\n",
    "class ModelEvaluationConfig:\n",
    "    def __init__(self,training_pipeline_config:TrainingPipelineConfig):\n",
    "        self.change_threshold = 0.01\n",
    "\n",
    "\n",
    "class ModelPusherConfig:\n",
    "\n",
    "    def __init__(self,training_pipeline_config:TrainingPipelineConfig):\n",
    "        self.model_pusher_dir = os.path.join(training_pipeline_config.artifact_dir , \"model_pusher\")\n",
    "        self.saved_model_dir = os.path.join(\"saved_models\")\n",
    "        self.pusher_model_dir = os.path.join(self.model_pusher_dir,\"saved_models\")\n",
    "        self.pusher_model_path = os.path.join(self.pusher_model_dir,MODEL_FILE_NAME)\n",
    "        self.pusher_transformer_path = os.path.join(self.pusher_model_dir,TRANSFORMER_OBJECT_FILE_NAME)\n",
    "        self.pusher_target_encoder_path = os.path.join(self.pusher_model_dir,TARGET_ENCODER_OBJECT_FILE_NAME)\n",
    "```\n",
    "\n",
    "### And Update the **`main.py`** file\n",
    "```python\n",
    "from Insurance.logger import logging\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.utils import get_collection_as_dataframe\n",
    "import sys, os\n",
    "from Insurance.entity.config_entity import DataIngestionConfig\n",
    "from Insurance.entity import config_entity\n",
    "from Insurance.components.data_ingestion import DataIngestion\n",
    "from Insurance.components.data_validation import DataValidation\n",
    "from Insurance.components.data_transformation import DataTransformation\n",
    "from Insurance.components.model_trainer import ModelTrainer\n",
    "from Insurance.components.model_evaluation import ModelEvaluation\n",
    "from Insurance.components.model_pusher import ModelPusher\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def test_logger_and_expection():\n",
    "   # try:\n",
    "       # logging.info(\"Starting the test_logger_and_exception\")\n",
    "        #result = 3/0\n",
    "       # print(result)\n",
    "       # logging.info(\"Stoping the test_logger_and_exception\")\n",
    "    #except Exception as e:\n",
    "      #  logging.debug(str(e))\n",
    "       # raise InsuranceException(e, sys)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "     try:\n",
    "          #start_training_pipeline()\n",
    "          #test_logger_and_expection()\n",
    "       # get_collection_as_dataframe(database_name =\"INSURANCE\", collection_name = 'INSURANCE_PROJECT')\n",
    "       training_pipeline_config = config_entity.TrainingPipelineConfig()\n",
    "       \n",
    "      #data ingestion\n",
    "       data_ingestion_config  = config_entity.DataIngestionConfig(training_pipeline_config=training_pipeline_config)\n",
    "       print(data_ingestion_config.to_dict())\n",
    "       data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "       data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\n",
    "       \n",
    "       #data validation\n",
    "       data_validation_config = config_entity.DataValidationConfig(training_pipeline_config=training_pipeline_config)\n",
    "       data_validation = DataValidation(data_validation_config=data_validation_config,\n",
    "                         data_ingestion_artifact=data_ingestion_artifact)\n",
    "        \n",
    "       data_validation_artifact = data_validation.initiate_data_validation()\n",
    "\n",
    "       # data transformation\n",
    "       data_transformation_config = config_entity.DataTransformationConfig(training_pipeline_config=training_pipeline_config)\n",
    "       data_transformation = DataTransformation(data_transformation_config=data_transformation_config, \n",
    "       data_ingestion_artifact=data_ingestion_artifact)\n",
    "       data_transformation_artifact = data_transformation.initiate_data_transformation()\n",
    "\n",
    "       #model trainer\n",
    "       model_trainer_config = config_entity.ModelTrainerConfig(training_pipeline_config=training_pipeline_config)\n",
    "       model_trainer = ModelTrainer(model_trainer_config=model_trainer_config, data_transformation_artifact=data_transformation_artifact)\n",
    "       model_trainer_artifact = model_trainer.initiate_model_trainer()\n",
    "\n",
    "\n",
    "      #model evaluation\n",
    "       model_eval_config = config_entity.ModelEvaluationConfig(training_pipeline_config=training_pipeline_config)\n",
    "       model_eval  = ModelEvaluation(model_eval_config=model_eval_config,\n",
    "       data_ingestion_artifact=data_ingestion_artifact,\n",
    "       data_transformation_artifact=data_transformation_artifact,\n",
    "       model_trainer_artifact=model_trainer_artifact)\n",
    "       model_eval_artifact = model_eval.initiate_model_evaluation()\n",
    "\n",
    "\n",
    "      # model pusher\n",
    "       model_pusher_config = config_entity.ModelPusherConfig(training_pipeline_config=training_pipeline_config)\n",
    "       model_pusher = ModelPusher(model_pusher_config=model_pusher_config,\n",
    "                                     data_transformation_artifact=data_transformation_artifact,\n",
    "                                     model_trainer_artifact=model_trainer_artifact)\n",
    "       model_pusher_artifact = model_pusher.initiate_model_pusher()\n",
    "\n",
    "     except Exception as e:\n",
    "          print(e)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part- 4 & 5\n",
    "## Create a folder **`components`** in the existing folder\n",
    "### Create a file **`__init__.py`** in the components folder\n",
    "\n",
    "### Create a file **`data_ingestion.py`** in the components folder and write the following code\n",
    "```python\n",
    "\n",
    "from Insurance import utils\n",
    "from Insurance.entity import config_entity\n",
    "from Insurance.entity import artifact_entity\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.logger import logging\n",
    "import os,sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataIngestion:\n",
    "    \n",
    "    def __init__(self,data_ingestion_config:config_entity.DataIngestionConfig ):\n",
    "        try:\n",
    "            self.data_ingestion_config = data_ingestion_config\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "    def initiate_data_ingestion(self)->artifact_entity.DataIngestionArtifact:\n",
    "        try:\n",
    "            logging.info(f\"Exporting collection data as pandas dataframe\")\n",
    "            #Exporting collection data as pandas dataframe\n",
    "            df:pd.DataFrame  = utils.get_collection_as_dataframe(\n",
    "                database_name=self.data_ingestion_config.database_name, \n",
    "                collection_name=self.data_ingestion_config.collection_name)\n",
    "\n",
    "            logging.info(\"Save data in feature store\")\n",
    "\n",
    "            #replace na with Nan\n",
    "            df.replace(to_replace=\"na\",value=np.NAN,inplace=True)\n",
    "\n",
    "            #Save data in feature store\n",
    "            logging.info(\"Create feature store folder if not available\")\n",
    "            #Create feature store folder if not available\n",
    "            feature_store_dir = os.path.dirname(self.data_ingestion_config.feature_store_file_path)\n",
    "            os.makedirs(feature_store_dir,exist_ok=True)\n",
    "            logging.info(\"Save df to feature store folder\")\n",
    "            #Save df to feature store folder\n",
    "            df.to_csv(path_or_buf=self.data_ingestion_config.feature_store_file_path,index=False,header=True)\n",
    "\n",
    "\n",
    "            logging.info(\"split dataset into train and test set\")\n",
    "            #split dataset into train and test set\n",
    "            train_df,test_df = train_test_split(df,test_size=self.data_ingestion_config.test_size, random_state = 1)\n",
    "            \n",
    "            logging.info(\"create dataset directory folder if not available\")\n",
    "            #create dataset directory folder if not available\n",
    "            dataset_dir = os.path.dirname(self.data_ingestion_config.train_file_path)\n",
    "            os.makedirs(dataset_dir,exist_ok=True)\n",
    "\n",
    "            logging.info(\"Save df to feature store folder\")\n",
    "            #Save df to feature store folder\n",
    "            train_df.to_csv(path_or_buf=self.data_ingestion_config.train_file_path,index=False,header=True)\n",
    "            test_df.to_csv(path_or_buf=self.data_ingestion_config.test_file_path,index=False,header=True)\n",
    "            \n",
    "            #Prepare artifact\n",
    "\n",
    "            data_ingestion_artifact = artifact_entity.DataIngestionArtifact(\n",
    "                feature_store_file_path=self.data_ingestion_config.feature_store_file_path,\n",
    "                train_file_path=self.data_ingestion_config.train_file_path, \n",
    "                test_file_path=self.data_ingestion_config.test_file_path)\n",
    "\n",
    "            logging.info(f\"Data ingestion artifact: {data_ingestion_artifact}\")\n",
    "            return data_ingestion_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(error_message=e, error_detail=sys)\n",
    "```\n",
    "### Create a file **`data_transformation.py`** in the components folder and write the following code\n",
    "```python\n",
    "from Insurance.entity import artifact_entity,config_entity\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.logger import logging\n",
    "from typing import Optional\n",
    "import os,sys \n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "from Insurance import utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from Insurance.config import TARGET_COLUMN\n",
    "\n",
    "# Missing values imputation\n",
    "# Outliers Handling\n",
    "# Imbalanced data handling\n",
    "# Convert Categorical data into numerical data\n",
    "\n",
    "class DataTransformation:\n",
    "\n",
    "\n",
    "    def __init__(self,data_transformation_config:config_entity.DataTransformationConfig,\n",
    "                    data_ingestion_artifact:artifact_entity.DataIngestionArtifact):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20} Data Transformation {'<<'*20}\")\n",
    "            self.data_transformation_config=data_transformation_config\n",
    "            self.data_ingestion_artifact=data_ingestion_artifact\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_data_transformer_object(cls)->Pipeline: # Create cls class\n",
    "        try:\n",
    "            simple_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "            robust_scaler =  RobustScaler()\n",
    "            pipeline = Pipeline(steps=[\n",
    "                    ('Imputer',simple_imputer),\n",
    "                    ('RobustScaler',robust_scaler)\n",
    "                ])\n",
    "            return pipeline\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "\n",
    "    def initiate_data_transformation(self,) -> artifact_entity.DataTransformationArtifact:\n",
    "        try:\n",
    "            #reading training and testing file\n",
    "            train_df = pd.read_csv(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\n",
    "            \n",
    "            #selecting input feature for train and test dataframe\n",
    "            input_feature_train_df=train_df.drop(TARGET_COLUMN,axis=1)\n",
    "            input_feature_test_df=test_df.drop(TARGET_COLUMN,axis=1)\n",
    "\n",
    "            #selecting target feature for train and test dataframe\n",
    "            target_feature_train_df = train_df[TARGET_COLUMN]\n",
    "            target_feature_test_df = test_df[TARGET_COLUMN]\n",
    "\n",
    "            label_encoder = LabelEncoder()\n",
    "            # label_encoder.fit(target_feature_train_df)\n",
    "\n",
    "            #transformation on target columns\n",
    "            target_feature_train_arr = target_feature_train_df.squeeze()\n",
    "            target_feature_test_arr = target_feature_test_df.squeeze()\n",
    "\n",
    "            #transformation on categorical columns\n",
    "            for col in input_feature_train_df.columns:\n",
    "                if input_feature_test_df[col].dtypes == 'O':\n",
    "                    input_feature_train_df[col] = label_encoder.fit_transform(input_feature_train_df[col])\n",
    "                    input_feature_test_df[col] = label_encoder.fit_transform(input_feature_test_df[col])\n",
    "                else:\n",
    "                    input_feature_train_df[col] = input_feature_train_df[col]\n",
    "                    input_feature_test_df[col] = input_feature_test_df[col]\n",
    "\n",
    "            \n",
    "            transformation_pipleine = DataTransformation.get_data_transformer_object()\n",
    "            transformation_pipleine.fit(input_feature_train_df)\n",
    "\n",
    "            #transforming input features\n",
    "            input_feature_train_arr = transformation_pipleine.transform(input_feature_train_df)\n",
    "            input_feature_test_arr = transformation_pipleine.transform(input_feature_test_df)\n",
    "            \n",
    "\n",
    "            # smt = SMOTETomek(random_state=42)\n",
    "            # logging.info(f\"Before resampling in training set Input: {input_feature_train_arr.shape} Target:{target_feature_train_arr.shape}\")\n",
    "            # input_feature_train_arr, target_feature_train_arr = smt.fit_resample(input_feature_train_arr, target_feature_train_arr)\n",
    "            # logging.info(f\"After resampling in training set Input: {input_feature_train_arr.shape} Target:{target_feature_train_arr.shape}\")\n",
    "            \n",
    "            # logging.info(f\"Before resampling in testing set Input: {input_feature_test_arr.shape} Target:{target_feature_test_arr.shape}\")\n",
    "            # input_feature_test_arr, target_feature_test_arr = smt.fit_resample(input_feature_test_arr, target_feature_test_arr)\n",
    "            # logging.info(f\"After resampling in testing set Input: {input_feature_test_arr.shape} Target:{target_feature_test_arr.shape}\")\n",
    "\n",
    "            #target encoder\n",
    "            train_arr = np.c_[input_feature_train_arr, target_feature_train_arr ]\n",
    "            test_arr = np.c_[input_feature_test_arr, target_feature_test_arr]\n",
    "\n",
    "\n",
    "            #save numpy array\n",
    "            utils.save_numpy_array_data(file_path=self.data_transformation_config.transformed_train_path,\n",
    "                                        array=train_arr)\n",
    "\n",
    "            utils.save_numpy_array_data(file_path=self.data_transformation_config.transformed_test_path,\n",
    "                                        array=test_arr)\n",
    "\n",
    "\n",
    "            utils.save_object(file_path=self.data_transformation_config.transform_object_path,\n",
    "             obj=transformation_pipleine)\n",
    "\n",
    "            utils.save_object(file_path=self.data_transformation_config.target_encoder_path,\n",
    "            obj=label_encoder)\n",
    "\n",
    "\n",
    "\n",
    "            data_transformation_artifact = artifact_entity.DataTransformationArtifact(\n",
    "                transform_object_path=self.data_transformation_config.transform_object_path,\n",
    "                transformed_train_path = self.data_transformation_config.transformed_train_path,\n",
    "                transformed_test_path = self.data_transformation_config.transformed_test_path,\n",
    "                target_encoder_path = self.data_transformation_config.target_encoder_path\n",
    "\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Data transformation object {data_transformation_artifact}\")\n",
    "            return data_transformation_artifact\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "```\n",
    "### Create a file **`data_validation.py`** in the components folder and write the following code\n",
    "```python\n",
    "from Insurance.entity import artifact_entity,config_entity\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.logger import logging\n",
    "from scipy.stats import ks_2samp\n",
    "from typing import Optional\n",
    "import os,sys \n",
    "import pandas as pd\n",
    "from Insurance import utils\n",
    "import numpy as np\n",
    "from Insurance.config import TARGET_COLUMN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataValidation:\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                    data_validation_config:config_entity.DataValidationConfig,\n",
    "                    data_ingestion_artifact:artifact_entity.DataIngestionArtifact):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20} Data Validation {'<<'*20}\")\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self.data_ingestion_artifact=data_ingestion_artifact\n",
    "            self.validation_error=dict()\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "        \n",
    "    \n",
    "\n",
    "    def drop_missing_values_columns(self,df:pd.DataFrame,report_key_name:str)->Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        This function will drop column which contains missing value more than specified threshold\n",
    "\n",
    "        df: Accepts a pandas dataframe\n",
    "        threshold: Percentage criteria to drop a column\n",
    "        =====================================================================================\n",
    "        returns Pandas DataFrame if atleast a single column is available after missing columns drop else None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            threshold = self.data_validation_config.missing_threshold\n",
    "            null_report = df.isna().sum()/df.shape[0]\n",
    "            #selecting column name which contains null\n",
    "            logging.info(f\"selecting column name which contains null above to {threshold}\")\n",
    "            drop_column_names = null_report[null_report>threshold].index\n",
    "\n",
    "            logging.info(f\"Columns to drop: {list(drop_column_names)}\")\n",
    "            self.validation_error[report_key_name]=list(drop_column_names)\n",
    "            df.drop(list(drop_column_names),axis=1,inplace=True)\n",
    "\n",
    "            #return None no columns left\n",
    "            if len(df.columns)==0:\n",
    "                return None\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "    #     print(\"dsgfg\")\n",
    "    def is_required_columns_exists(self,base_df:pd.DataFrame,current_df:pd.DataFrame,report_key_name:str)->bool:\n",
    "        try:\n",
    "           \n",
    "            base_columns = base_df.columns\n",
    "            current_columns = current_df.columns\n",
    "\n",
    "            missing_columns = []\n",
    "            for base_column in base_columns:\n",
    "                if base_column not in current_columns:\n",
    "                    logging.info(f\"Column: [{base} is not available.]\")\n",
    "                    missing_columns.append(base_column)\n",
    "            \n",
    "            if len(missing_columns)>0:\n",
    "                self.validation_error[report_key_name]=missing_columns\n",
    "                return False\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "    def data_drift(self,base_df:pd.DataFrame,current_df:pd.DataFrame,report_key_name:str):\n",
    "        try:\n",
    "            drift_report=dict()\n",
    "\n",
    "            base_columns = base_df.columns\n",
    "            current_columns = current_df.columns\n",
    "\n",
    "            for base_column in base_columns:\n",
    "                base_data,current_data = base_df[base_column],current_df[base_column]\n",
    "                #Null hypothesis is that both column data drawn from same distrubtion\n",
    "                \n",
    "                logging.info(f\"Hypothesis {base_column}: {base_data.dtype}, {current_data.dtype} \")\n",
    "                same_distribution =ks_2samp(base_data,current_data)\n",
    "\n",
    "                if same_distribution.pvalue>0.05:\n",
    "                    #We are accepting null hypothesis\n",
    "                    drift_report[base_column]={\n",
    "                        \"pvalues\":float(same_distribution.pvalue),\n",
    "                        \"same_distribution\": True\n",
    "                    }\n",
    "                else:\n",
    "                    drift_report[base_column]={\n",
    "                        \"pvalues\":float(same_distribution.pvalue),\n",
    "                        \"same_distribution\":False\n",
    "                    }\n",
    "                    #different distribution\n",
    "\n",
    "            self.validation_error[report_key_name]=drift_report\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "    def initiate_data_validation(self)->artifact_entity.DataValidationArtifact:\n",
    "        try:\n",
    "            logging.info(f\"Reading base dataframe\")\n",
    "            base_df = pd.read_csv(self.data_validation_config.base_file_path)\n",
    "            base_df.replace({\"na\":np.NAN},inplace=True)\n",
    "            logging.info(f\"Replace na value in base df\")\n",
    "            #base_df has na as null\n",
    "            logging.info(f\"Drop null values colums from base df\")\n",
    "            base_df=self.drop_missing_values_columns(df=base_df,report_key_name=\"missing_values_within_base_dataset\")\n",
    "\n",
    "            logging.info(f\"Reading train dataframe\")\n",
    "            train_df = pd.read_csv(self.data_ingestion_artifact.train_file_path)\n",
    "            logging.info(f\"Reading test dataframe\")\n",
    "            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\n",
    "\n",
    "            logging.info(f\"Drop null values colums from train df\")\n",
    "            train_df = self.drop_missing_values_columns(df=train_df,report_key_name=\"missing_values_within_train_dataset\")\n",
    "            logging.info(f\"Drop null values colums from test df\")\n",
    "            test_df = self.drop_missing_values_columns(df=test_df,report_key_name=\"missing_values_within_test_dataset\")\n",
    "            \n",
    "            exclude_columns = [TARGET_COLUMN]\n",
    "            base_df = utils.convert_columns_float(df=base_df, exclude_columns=exclude_columns)\n",
    "            train_df = utils.convert_columns_float(df=train_df, exclude_columns=exclude_columns)\n",
    "            test_df = utils.convert_columns_float(df=test_df, exclude_columns=exclude_columns)\n",
    "\n",
    "\n",
    "            logging.info(f\"Is all required columns present in train df\")\n",
    "            train_df_columns_status = self.is_required_columns_exists(base_df=base_df, current_df=train_df,report_key_name=\"missing_columns_within_train_dataset\")\n",
    "            logging.info(f\"Is all required columns present in test df\")\n",
    "            test_df_columns_status = self.is_required_columns_exists(base_df=base_df, current_df=test_df,report_key_name=\"missing_columns_within_test_dataset\")\n",
    "\n",
    "            if train_df_columns_status:\n",
    "                logging.info(f\"As all column are available in train df hence detecting data drift\")\n",
    "                self.data_drift(base_df=base_df, current_df=train_df,report_key_name=\"data_drift_within_train_dataset\")\n",
    "            if test_df_columns_status:\n",
    "                logging.info(f\"As all column are available in test df hence detecting data drift\")\n",
    "                self.data_drift(base_df=base_df, current_df=test_df,report_key_name=\"data_drift_within_test_dataset\")\n",
    "          \n",
    "            #write the report\n",
    "            logging.info(\"Write reprt in yaml file\")\n",
    "            utils.write_yaml_file(file_path=self.data_validation_config.report_file_path,\n",
    "            data=self.validation_error)\n",
    "\n",
    "            data_validation_artifact = artifact_entity.DataValidationArtifact(report_file_path=self.data_validation_config.report_file_path,)\n",
    "            logging.info(f\"Data validation artifact: {data_validation_artifact}\")\n",
    "            return data_validation_artifact\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "```\n",
    "### Create a file **`data_evaluation.py`** in the components folder and write the following code\n",
    "```python\n",
    "from Insurance.predictor import ModelResolver\n",
    "from Insurance.entity import config_entity,artifact_entity\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.logger import logging\n",
    "from Insurance.utils import load_object\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas  as pd\n",
    "import sys,os\n",
    "from Insurance.config import TARGET_COLUMN\n",
    "\n",
    "class ModelEvaluation:\n",
    "\n",
    "    def __init__(self,\n",
    "        model_eval_config:config_entity.ModelEvaluationConfig,\n",
    "        data_ingestion_artifact:artifact_entity.DataIngestionArtifact,\n",
    "        data_transformation_artifact:artifact_entity.DataTransformationArtifact,\n",
    "        model_trainer_artifact:artifact_entity.ModelTrainerArtifact      \n",
    "        ):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20}  Model Evaluation {'<<'*20}\")\n",
    "            self.model_eval_config=model_eval_config\n",
    "            self.data_ingestion_artifact=data_ingestion_artifact\n",
    "            self.data_transformation_artifact=data_transformation_artifact\n",
    "            self.model_trainer_artifact=model_trainer_artifact\n",
    "            self.model_resolver = ModelResolver()\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e,sys)\n",
    "\n",
    "\n",
    "\n",
    "    def initiate_model_evaluation(self)->artifact_entity.ModelEvaluationArtifact:\n",
    "        try:\n",
    "            #if saved model folder has model the we will compare \n",
    "            #which model is best trained or the model from saved model folder\n",
    "\n",
    "            logging.info(\"if saved model folder has model the we will compare \"\n",
    "            \"which model is best trained or the model from saved model folder\")\n",
    "            latest_dir_path = self.model_resolver.get_latest_dir_path()\n",
    "            if latest_dir_path==None:\n",
    "                model_eval_artifact = artifact_entity.ModelEvaluationArtifact(is_model_accepted=True,\n",
    "                improved_accuracy=None)\n",
    "                logging.info(f\"Model evaluation artifact: {model_eval_artifact}\")\n",
    "                return model_eval_artifact\n",
    "\n",
    "\n",
    "            #Finding location of transformer model and target encoder\n",
    "            logging.info(\"Finding location of transformer model and target encoder\")\n",
    "            transformer_path = self.model_resolver.get_latest_transformer_path()\n",
    "            model_path = self.model_resolver.get_latest_model_path()\n",
    "            target_encoder_path = self.model_resolver.get_latest_target_encoder_path()\n",
    "\n",
    "            logging.info(\"Previous trained objects of transformer, model and target encoder\")\n",
    "            #Previous trained  objects\n",
    "            transformer = load_object(file_path=transformer_path)\n",
    "            model = load_object(file_path=model_path)\n",
    "            target_encoder = load_object(file_path=target_encoder_path)\n",
    "            \n",
    "\n",
    "            logging.info(\"Currently trained model objects\")\n",
    "            #Currently trained model objects\n",
    "            current_transformer = load_object(file_path=self.data_transformation_artifact.transform_object_path)\n",
    "            current_model  = load_object(file_path=self.model_trainer_artifact.model_path)\n",
    "            current_target_encoder = load_object(file_path=self.data_transformation_artifact.target_encoder_path)\n",
    "            \n",
    "\n",
    "            # take tyest data for testing test data \n",
    "\n",
    "            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\n",
    "            target_df = test_df[TARGET_COLUMN]\n",
    "            y_true = target_df\n",
    "            # target_encoder.transform(target_df)\n",
    "            # accuracy using previous trained model\n",
    "            \n",
    "            \"\"\"We need to create label encoder object for each categorical variable. We will check later\"\"\"\n",
    "            input_feature_name = list(transformer.feature_names_in_)\n",
    "            for i in input_feature_name:       \n",
    "                if test_df[i].dtypes =='object':\n",
    "                    test_df[i] =target_encoder.fit_transform(test_df[i])  \n",
    "\n",
    "            input_arr =transformer.transform(test_df[input_feature_name])\n",
    "            y_pred = model.predict(input_arr)\n",
    "            print(f\"Prediction using previous model: {y_pred[:5]}\")\n",
    "            previous_model_score = r2_score(y_true=y_true, y_pred=y_pred)\n",
    "            logging.info(f\"Accuracy using previous trained model: {previous_model_score}\")\n",
    "           \n",
    "            # accuracy using current trained model\n",
    "            input_feature_name = list(current_transformer.feature_names_in_)\n",
    "            input_arr =current_transformer.transform(test_df[input_feature_name])\n",
    "            y_pred = current_model.predict(input_arr)\n",
    "            y_true = target_df\n",
    "            # current_target_encoder.transform(target_df)\n",
    "            # current_target_encoder.inverse_transform(y_pred[:5])\n",
    "            print(f\"Prediction using trained model: {y_pred[:5]}\")\n",
    "            current_model_score = r2_score(y_true=y_true, y_pred=y_pred)\n",
    "            logging.info(f\"Accuracy using current trained model: {current_model_score}\")\n",
    "            if current_model_score<=previous_model_score:\n",
    "                logging.info(f\"Current trained model is not better than previous model\")\n",
    "                raise Exception(\"Current trained model is not better than previous model\")\n",
    "\n",
    "            \n",
    "            model_eval_artifact = artifact_entity.ModelEvaluationArtifact(is_model_accepted=True,\n",
    "            improved_accuracy=current_model_score-previous_model_score)\n",
    "            logging.info(f\"Model eval artifact: {model_eval_artifact}\")\n",
    "            return model_eval_artifact\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e,sys)\n",
    "```\n",
    "### Create a file **`data_pusher.py`** in the components folder and write the following code\n",
    "```python\n",
    "from Insurance.predictor import ModelResolver\n",
    "from Insurance.entity.config_entity import ModelPusherConfig\n",
    "from Insurance.exception import InsuranceException\n",
    "import os,sys\n",
    "from Insurance.utils import load_object,save_object\n",
    "from Insurance.logger import logging\n",
    "from Insurance.entity.artifact_entity import DataTransformationArtifact,ModelTrainerArtifact,ModelPusherArtifact\n",
    "\n",
    "class ModelPusher:\n",
    "\n",
    "    def __init__(self,model_pusher_config:ModelPusherConfig,\n",
    "    data_transformation_artifact:DataTransformationArtifact,\n",
    "    model_trainer_artifact:ModelTrainerArtifact):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20} Data Transformation {'<<'*20}\")\n",
    "            self.model_pusher_config=model_pusher_config\n",
    "            self.data_transformation_artifact=data_transformation_artifact\n",
    "            self.model_trainer_artifact=model_trainer_artifact\n",
    "            self.model_resolver = ModelResolver(model_registry=self.model_pusher_config.saved_model_dir)\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "    def initiate_model_pusher(self,)->ModelPusherArtifact:\n",
    "        try:\n",
    "            #load object\n",
    "            logging.info(f\"Loading transformer model and target encoder\")\n",
    "            transformer = load_object(file_path=self.data_transformation_artifact.transform_object_path)\n",
    "            model = load_object(file_path=self.model_trainer_artifact.model_path)\n",
    "            target_encoder = load_object(file_path=self.data_transformation_artifact.target_encoder_path)\n",
    "\n",
    "            #model pusher dir\n",
    "            logging.info(f\"Saving model into model pusher directory\")\n",
    "            save_object(file_path=self.model_pusher_config.pusher_transformer_path, obj=transformer)\n",
    "            save_object(file_path=self.model_pusher_config.pusher_model_path, obj=model)\n",
    "            save_object(file_path=self.model_pusher_config.pusher_target_encoder_path, obj=target_encoder)\n",
    "\n",
    "\n",
    "            #saved model dir\n",
    "            logging.info(f\"Saving model in saved model dir\")\n",
    "            transformer_path=self.model_resolver.get_latest_save_transformer_path()\n",
    "            model_path=self.model_resolver.get_latest_save_model_path()\n",
    "            target_encoder_path=self.model_resolver.get_latest_save_target_encoder_path()\n",
    "\n",
    "            save_object(file_path=transformer_path, obj=transformer)\n",
    "            save_object(file_path=model_path, obj=model)\n",
    "            save_object(file_path=target_encoder_path, obj=target_encoder)\n",
    "\n",
    "            model_pusher_artifact = ModelPusherArtifact(pusher_model_dir=self.model_pusher_config.pusher_model_dir,\n",
    "             saved_model_dir=self.model_pusher_config.saved_model_dir)\n",
    "            logging.info(f\"Model pusher artifact: {model_pusher_artifact}\")\n",
    "            return model_pusher_artifact\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "```\n",
    "### Create a file **`data_trainer.py`** in the components folder and write the following code\n",
    "```python\n",
    "from Insurance.entity import artifact_entity,config_entity\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.logger import logging\n",
    "from typing import Optional\n",
    "import os,sys \n",
    "import xgboost as xg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from Insurance import utils\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "\n",
    "\n",
    "    def __init__(self,model_trainer_config:config_entity.ModelTrainerConfig,\n",
    "                data_transformation_artifact:artifact_entity.DataTransformationArtifact\n",
    "                ):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20} Model Trainer {'<<'*20}\")\n",
    "            self.model_trainer_config=model_trainer_config\n",
    "            self.data_transformation_artifact=data_transformation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "    def fine_tune(self):\n",
    "        try:\n",
    "            #Wite code for Grid Search CV\n",
    "            pass\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "    def train_model(self,x,y):\n",
    "        # try:\n",
    "        #     xgb_r = xg.XGBRegressor()\n",
    "        #     xgb_r.fit(x,y)\n",
    "        #     return xgb_r\n",
    "        # except Exception as e:\n",
    "        #     raise InsuranceException(e, sys)\n",
    "\n",
    "        try:\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(x,y)\n",
    "            return lr\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "\n",
    "\n",
    "\n",
    "    def initiate_model_trainer(self,)->artifact_entity.ModelTrainerArtifact:\n",
    "        try:\n",
    "            logging.info(f\"Loading train and test array.\")\n",
    "            train_arr = utils.load_numpy_array_data(file_path=self.data_transformation_artifact.transformed_train_path)\n",
    "            test_arr = utils.load_numpy_array_data(file_path=self.data_transformation_artifact.transformed_test_path)\n",
    "\n",
    "            logging.info(f\"Splitting input and target feature from both train and test arr.\")\n",
    "            x_train,y_train = train_arr[:,:-1],train_arr[:,-1]\n",
    "            x_test,y_test = test_arr[:,:-1],test_arr[:,-1]\n",
    "\n",
    "            logging.info(f\"Train the model\")\n",
    "            model = self.train_model(x=x_train,y=y_train)\n",
    "\n",
    "            logging.info(f\"Calculating f1 train score\")\n",
    "            yhat_train = model.predict(x_train)\n",
    "            r2_train_score  =r2_score(y_true=y_train, y_pred=yhat_train)\n",
    "\n",
    "            logging.info(f\"Calculating f1 test score\")\n",
    "            yhat_test = model.predict(x_test)\n",
    "            r2_test_score  =r2_score(y_true=y_test, y_pred=yhat_test)\n",
    "            \n",
    "            logging.info(f\"train score:{r2_train_score} and tests score {r2_test_score}\")\n",
    "            #check for overfitting or underfiiting or expected score\n",
    "            logging.info(f\"Checking if our model is underfitting or not\")\n",
    "            if r2_test_score<self.model_trainer_config.expected_score:\n",
    "                raise Exception(f\"Model is not good as it is not able to give \\\n",
    "                expected accuracy: {self.model_trainer_config.expected_score}: model actual score: {r2_test_score}\")\n",
    "\n",
    "            logging.info(f\"Checking if our model is overfiiting or not\")\n",
    "            diff = abs(r2_train_score-r2_test_score)\n",
    "\n",
    "            if diff>self.model_trainer_config.overfitting_threshold:\n",
    "                raise Exception(f\"Train and test score diff: {diff} is more than overfitting threshold {self.model_trainer_config.overfitting_threshold}\")\n",
    "\n",
    "            #save the trained model\n",
    "            logging.info(f\"Saving mode object\")\n",
    "            utils.save_object(file_path=self.model_trainer_config.model_path, obj=model)\n",
    "\n",
    "            #prepare artifact\n",
    "            logging.info(f\"Prepare the artifact\")\n",
    "            model_trainer_artifact  = artifact_entity.ModelTrainerArtifact(model_path=self.model_trainer_config.model_path, \n",
    "            r2_train_score=r2_train_score, r2_test_score=r2_test_score)\n",
    "            logging.info(f\"Model trainer artifact: {model_trainer_artifact}\")\n",
    "            return model_trainer_artifact\n",
    "        except Exception as e:\n",
    "            raise InsuranceException(e, sys)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub Commands\n",
    "```github\n",
    "git add .\n",
    "git commit -m \"data componenets\"\n",
    "git push -u origin main\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 6\n",
    "## Update **`main.py`** file of Insurance folder\n",
    "## Run **`main.py`** file \n",
    "\n",
    "## Create a file **`predictor.py`** in **`Insurance`** folder write the following code\n",
    "```python\n",
    "import os\n",
    "from Insurance.entity.config_entity import TRANSFORMER_OBJECT_FILE_NAME,MODEL_FILE_NAME,TARGET_ENCODER_OBJECT_FILE_NAME\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "# Now lets start model validation\n",
    "\n",
    "\n",
    "class ModelResolver:\n",
    "    \n",
    "    def __init__(self,model_registry:str = \"saved_models\",\n",
    "                transformer_dir_name=\"transformer\",\n",
    "                target_encoder_dir_name = \"target_encoder\",\n",
    "                model_dir_name = \"model\"):\n",
    "\n",
    "        self.model_registry=model_registry\n",
    "        os.makedirs(self.model_registry,exist_ok=True)\n",
    "        self.transformer_dir_name = transformer_dir_name\n",
    "        self.target_encoder_dir_name=target_encoder_dir_name\n",
    "        self.model_dir_name=model_dir_name\n",
    "\n",
    "# 1\n",
    "    def get_latest_dir_path(self)->Optional[str]:\n",
    "        try:\n",
    "            dir_names = os.listdir(self.model_registry)\n",
    "            if len(dir_names)==0:\n",
    "                return None\n",
    "            dir_names = list(map(int,dir_names))\n",
    "            latest_dir_name = max(dir_names)\n",
    "            return os.path.join(self.model_registry,f\"{latest_dir_name}\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "# 2\n",
    "\n",
    "    def get_latest_model_path(self):\n",
    "        try:\n",
    "            latest_dir = self.get_latest_dir_path()\n",
    "            if latest_dir is None:\n",
    "                raise Exception(f\"Model is not available\")\n",
    "            return os.path.join(latest_dir,self.model_dir_name,MODEL_FILE_NAME)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "# 3\n",
    "    def get_latest_transformer_path(self):\n",
    "        try:\n",
    "            latest_dir = self.get_latest_dir_path()\n",
    "            if latest_dir is None:\n",
    "                raise Exception(f\"Transformer is not available\")\n",
    "            return os.path.join(latest_dir,self.transformer_dir_name,TRANSFORMER_OBJECT_FILE_NAME)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "# 4\n",
    "    def get_latest_target_encoder_path(self):\n",
    "        try:\n",
    "            latest_dir = self.get_latest_dir_path()\n",
    "            if latest_dir is None:\n",
    "                raise Exception(f\"Target encoder is not available\")\n",
    "            return os.path.join(latest_dir,self.target_encoder_dir_name,TARGET_ENCODER_OBJECT_FILE_NAME)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "# 5\n",
    "    def get_latest_save_dir_path(self)->str:\n",
    "        try:\n",
    "            latest_dir = self.get_latest_dir_path()\n",
    "            if latest_dir==None:\n",
    "                return os.path.join(self.model_registry,f\"{0}\")\n",
    "            latest_dir_num = int(os.path.basename(self.get_latest_dir_path()))\n",
    "            return os.path.join(self.model_registry,f\"{latest_dir_num+1}\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "# 6\n",
    "    def get_latest_save_model_path(self):\n",
    "        try:\n",
    "            latest_dir = self.get_latest_save_dir_path()\n",
    "            return os.path.join(latest_dir,self.model_dir_name,MODEL_FILE_NAME)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "# 7\n",
    "    def get_latest_save_transformer_path(self):\n",
    "        try:\n",
    "            latest_dir = self.get_latest_save_dir_path()\n",
    "            return os.path.join(latest_dir,self.transformer_dir_name,TRANSFORMER_OBJECT_FILE_NAME)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "# 8\n",
    "    def get_latest_save_target_encoder_path(self):\n",
    "        try:\n",
    "            latest_dir = self.get_latest_save_dir_path()\n",
    "            return os.path.join(latest_dir,self.target_encoder_dir_name,TARGET_ENCODER_OBJECT_FILE_NAME)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "```\n",
    "\n",
    "## Rum **`main.py`** file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 7\n",
    "### GitHub Commands\n",
    "```github\n",
    "git add .\n",
    "git commit -m \"Updated predictor file\"\n",
    "git push -u origin main\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 8\n",
    "## Crete a **`batch_prediction.py`** in pipeline folder of Insurance folder\n",
    "```python\n",
    "import numpy as np\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.logger import logging\n",
    "from Insurance.predictor import ModelResolver\n",
    "import pandas as pd\n",
    "from Insurance.utils import load_object\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "PREDICTION_DIR = \"prediction\"\n",
    "\n",
    "\n",
    "def start_batch_prediction(input_file_path):\n",
    "    try:\n",
    "        os.makedirs(PREDICTION_DIR, exist_ok=True)\n",
    "        logging.info(f\"Creating model resolver object\")\n",
    "        model_resolver = ModelResolver(model_registry=\"saved_models\")\n",
    "        logging.info(f\"Reading file :{input_file_path}\")\n",
    "        df = pd.read_csv(input_file_path)\n",
    "        df.replace({\"na\": np.NAN}, inplace=True)\n",
    "        # validation\n",
    "\n",
    "        logging.info(f\"Loading transformer to transform dataset\")\n",
    "        transformer = load_object(\n",
    "            file_path=model_resolver.get_latest_transformer_path())\n",
    "\n",
    "       \n",
    "        logging.info(f\"Target encoder to convert predicted column into categorical\")\n",
    "        target_encoder = load_object(file_path=model_resolver.get_latest_target_encoder_path())\n",
    "\n",
    "\n",
    "        \"\"\"We need to create label encoder object for each categorical variable. We will check later\"\"\"\n",
    "        input_feature_names = list(transformer.feature_names_in_)\n",
    "        for i in input_feature_names:       \n",
    "            if df[i].dtypes =='object':\n",
    "                df[i] =target_encoder.fit_transform(df[i])  \n",
    "                    \n",
    "        input_arr = transformer.transform(df[input_feature_names])\n",
    "\n",
    "        logging.info(f\"Loading model to make prediction\")\n",
    "        model = load_object(file_path=model_resolver.get_latest_model_path())\n",
    "        prediction = model.predict(input_arr)\n",
    "        \n",
    "\n",
    "        # cat_prediction = target_encoder.inverse_transform(prediction)\n",
    "\n",
    "        df[\"prediction\"]=prediction\n",
    "        # df[\"cat_pred\"]=cat_prediction\n",
    "\n",
    "\n",
    "        prediction_file_name = os.path.basename(input_file_path).replace(\".csv\",f\"{datetime.now().strftime('%m%d%Y__%H%M%S')}.csv\")\n",
    "        prediction_file_path = os.path.join(PREDICTION_DIR,prediction_file_name)\n",
    "        df.to_csv(prediction_file_path,index=False,header=True)\n",
    "        return prediction_file_path\n",
    "    except Exception as e:\n",
    "        raise InsuranceException(e, sys)\n",
    "```\n",
    "## Update **`data_prediction.py`** file\n",
    "\n",
    "## Run **`main.py`** file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create **`demo.py`** file and write the following code\n",
    "```python\n",
    "from Insurance.pipeline.batch_prediction import start_batch_prediction\n",
    "from Insurance.pipeline.training_pipeline import start_training_pipeline\n",
    "\n",
    "file_path=r\"E:/wasiq/Insurance_Project/insurance.csv\"\n",
    "print(__name__)\n",
    "if __name__==\"__main__\":\n",
    "    try:\n",
    "        #output_file = start_training_pipeline()\n",
    "        output_file = start_batch_prediction(input_file_path=file_path)\n",
    "        print(output_file)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "```\n",
    "### Run **`demo.py`** file\n",
    "\n",
    "## Create **`training_pipeline.py`** file in the pipeline folder of Insurance and write the following code\n",
    "```python\n",
    "from Insurance.logger import logging\n",
    "from Insurance.exception import InsuranceException\n",
    "from Insurance.utils import get_collection_as_dataframe\n",
    "import sys,os\n",
    "from Insurance.entity import config_entity\n",
    "from Insurance.components.data_ingestion import DataIngestion\n",
    "from Insurance.components.data_validation import DataValidation\n",
    "from Insurance.components.data_transformation import DataTransformation\n",
    "from Insurance.components.model_trainer import ModelTrainer\n",
    "from Insurance.components.model_evaluation import ModelEvaluation\n",
    "from Insurance.components.model_pusher import ModelPusher\n",
    "\n",
    "\n",
    "def start_training_pipeline():\n",
    "    try:\n",
    "        training_pipeline_config = config_entity.TrainingPipelineConfig()\n",
    "\n",
    "        #data ingestion\n",
    "        data_ingestion_config  = config_entity.DataIngestionConfig(training_pipeline_config=training_pipeline_config)\n",
    "        print(data_ingestion_config.to_dict())\n",
    "        data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "        data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\n",
    "        \n",
    "        #data validation\n",
    "        data_validation_config = config_entity.DataValidationConfig(training_pipeline_config=training_pipeline_config)\n",
    "        data_validation = DataValidation(data_validation_config=data_validation_config,\n",
    "                        data_ingestion_artifact=data_ingestion_artifact)\n",
    "\n",
    "        data_validation_artifact = data_validation.initiate_data_validation()\n",
    "\n",
    "        #data transformation\n",
    "        data_transformation_config = config_entity.DataTransformationConfig(training_pipeline_config=training_pipeline_config)\n",
    "        data_transformation = DataTransformation(data_transformation_config=data_transformation_config, \n",
    "        data_ingestion_artifact=data_ingestion_artifact)\n",
    "        data_transformation_artifact = data_transformation.initiate_data_transformation()\n",
    "        \n",
    "        #model trainer\n",
    "        model_trainer_config = config_entity.ModelTrainerConfig(training_pipeline_config=training_pipeline_config)\n",
    "        model_trainer = ModelTrainer(model_trainer_config=model_trainer_config, data_transformation_artifact=data_transformation_artifact)\n",
    "        model_trainer_artifact = model_trainer.initiate_model_trainer()\n",
    "\n",
    "        #model evaluation\n",
    "        model_eval_config = config_entity.ModelEvaluationConfig(training_pipeline_config=training_pipeline_config)\n",
    "        model_eval  = ModelEvaluation(model_eval_config=model_eval_config,\n",
    "        data_ingestion_artifact=data_ingestion_artifact,\n",
    "        data_transformation_artifact=data_transformation_artifact,\n",
    "        model_trainer_artifact=model_trainer_artifact)\n",
    "        model_eval_artifact = model_eval.initiate_model_evaluation()\n",
    "\n",
    "        #model pusher\n",
    "        model_pusher_config = config_entity.ModelPusherConfig(training_pipeline_config)\n",
    "        model_pusher = ModelPusher(model_pusher_config=model_pusher_config, \n",
    "                data_transformation_artifact=data_transformation_artifact,\n",
    "                model_trainer_artifact=model_trainer_artifact)\n",
    "        model_pusher_artifact = model_pusher.initiate_model_pusher()\n",
    "    except Exception as e:\n",
    "        raise InsuranceException(e, sys)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit code for Application\n",
    "## Create a file **`app.py`**\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "# import preprocessor,helper\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import xgboost as xg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "model = pickle.load(open('model_final.pkl','rb'))\n",
    "encoder = pickle.load(open('target_encoder.pkl','rb'))\n",
    "transformer = pickle.load(open('transformer.pkl','rb'))\n",
    "\n",
    "st.title(\"Insurance Premium Prediction\")\n",
    "age = st.text_input('Enter Age', 18)\n",
    "age = int(age)\n",
    "\n",
    "sex = st.selectbox(\n",
    "    'Please select gender',\n",
    "    ('male', 'female'))\n",
    "# gender = encoder.transform(np.array([sex]))\n",
    "\n",
    "bmi = st.text_input('Enter BMI', 18)\n",
    "bmi = float(bmi)\n",
    "\n",
    "children = st.selectbox(\n",
    "    'Please select number of children ',\n",
    "    (0,1,2,3,4,5))\n",
    "children = int(children)\n",
    "\n",
    "\n",
    "smoker = st.selectbox(\n",
    "    'Please select smoker category ',\n",
    "    (\"yes\",\"no\"))\n",
    "# smoker = encoder.transform(smoker)\n",
    "\n",
    "region = st.selectbox(\n",
    "    'Please select region ',\n",
    "    (\"southwest\", \"southeast\", \"northeast\", \"northwest\"))\n",
    "\n",
    "\n",
    "l = {}\n",
    "l['age'] = age\n",
    "l['sex'] = sex\n",
    "l['bmi'] = bmi\n",
    "l['children'] = children\n",
    "l['smoker'] = smoker\n",
    "l['region'] = region\n",
    "\n",
    "df = pd.DataFrame(l, index=[0])\n",
    "\n",
    "df['region'] = encoder.transform(df['region'])\n",
    "df['sex'] = df['sex'].map({'male':1, 'female':0})\n",
    "df['smoker'] = df['smoker'].map({'yes':1, 'no':0})\n",
    "\n",
    "df = transformer.transform(df)\n",
    "# dtrain = xg.DMatrix(df)\n",
    "y_pred = model.predict(df)\n",
    "# st.write(age, gender, bmi, children, smoker, region)\n",
    "\n",
    "if st.button(\"Show Result\"):\n",
    "    # col1,col2, col3,col4 = st.columns(4)\n",
    "    st.header(f\"{round(y_pred[0],2)} INR\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b98f472bb8ba48098397e3b897b5be76f7bf0e62d98845cdb0e8066dc5677259"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
